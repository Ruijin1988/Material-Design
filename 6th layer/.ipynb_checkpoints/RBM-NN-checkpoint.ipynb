{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import random\n",
    "\n",
    "# import 5th layer data\n",
    "force = scipy.io.loadmat('true.mat')\n",
    "neurons = scipy.io.loadmat('hidstates5th_WB_recon_30neuron')\n",
    "neuron5=neurons['xtr'].T\n",
    "data_label=np.asarray(force['skt']).reshape(-1)\n",
    "label_y=(data_label-np.mean(data_label))/np.std(data_label)\n",
    "\n",
    "#import 4th layer data\n",
    "neurons = scipy.io.loadmat('hidstates4th_WB_recon_1000neuron')\n",
    "neuron4=neurons['xtr']\n",
    "W_4to5=scipy.io.loadmat('W5.mat')['W5']\n",
    "\n",
    "#import 3rd layer data\n",
    "neurons = scipy.io.loadmat('hidstates3rd_WB_recon_373248neuron')\n",
    "neuron3=neurons['xtr']\n",
    "W_3to4=scipy.io.loadmat('W4.mat')['W4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(373248L, 1000L)\n",
      "(1000L, 30L)\n"
     ]
    }
   ],
   "source": [
    "print W_3to4.shape\n",
    "print W_4to5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RBM_NN(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.W = theano.shared(value=np.zeros((n_in, n_out),dtype=theano.config.floatX),name='W',borrow=True)\n",
    "        self.b = theano.shared(value=np.zeros((n_out,),dtype=theano.config.floatX),name='b',borrow=True)               \n",
    "        self.y_pred = T.dot(input, self.W)+self.b      \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def difference(self,y):\n",
    "        return T.mean(abs(self.y_pred-T.reshape(y,[10,1])))\n",
    "    \n",
    "def load_data(data1, data2):\n",
    "    def shared_dataset(data1,data2,borrow=True):\n",
    "        shared_x = theano.shared(np.asarray(data1, dtype=theano.config.floatX),borrow=True)\n",
    "        shared_y = theano.shared(np.asarray(data2, dtype=theano.config.floatX),borrow=True)\n",
    "        return shared_x, T.cast(shared_y,'float64')\n",
    "    \n",
    "    train_set_x, train_set_y = shared_dataset(data1,data2)\n",
    "    rval = [(train_set_x, train_set_y)]\n",
    "    return rval\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self,input,n_in,n_out,WW):\n",
    "        self.input=input\n",
    "        W=np.asarray(WW,dtype=theano.config.floatX)\n",
    "        W=theano.shared(value=W, name='W',borrow=True)\n",
    "        #W_values = np.ones((n_in,n_out), dtype=theano.config.floatX)\n",
    "        #W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "                \n",
    "        b_value = np.zeros((n_out,),dtype=theano.config.floatX)\n",
    "        b = theano.shared(value=b_value, name='b',borrow=True)\n",
    "        \n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        \n",
    "        lin_output = T.dot(input,self.W) + self.b\n",
    "        self.output = T.nnet.sigmoid(lin_output)\n",
    "        \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        \n",
    "class MLP(object):\n",
    "    def __init__(self,input,n_in,n_hidden4,n_hidden,n_out):\n",
    "        \n",
    "        self.hiddenLayer3 = HiddenLayer(\n",
    "        input=input, n_in=n_in, n_out=n_hidden4,WW=W_3to4)\n",
    "        \n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "        input=self.hiddenLayer3.output, n_in=n_hidden4, n_out=n_hidden,WW=W_4to5)\n",
    "        \n",
    "        self.layer6 = RBM_NN(\n",
    "        input=self.hiddenLayer.output, n_in=n_hidden, n_out=n_out)\n",
    "        \n",
    "        \n",
    "        self.error = self.layer6.difference\n",
    "        \n",
    "        self.WD = abs(self.hiddenLayer3.W**2).sum() + abs(self.hiddenLayer.W**2).sum() + abs(self.layer6.W**2).sum()\n",
    "        \n",
    "        #self.error2 = abs(self.hiddenLayer.W).sum()+abs(self.layer6.W).sum()        \n",
    "        #self.error3 = abs(self.hiddenLayer3.W).sum()+abs(self.hiddenLayer.W).sum()+abs(self.layer6.W).sum()\n",
    "        \n",
    "        self.params = self.hiddenLayer3.params + self.hiddenLayer.params + self.layer6.params\n",
    "        \n",
    "        self.input = input\n",
    "\n",
    "def test_mlp(learning_rate=0.1, n_epochs=2000, batch_size=10, n_hidden4=1000, n_hidden=30, data1=neuron3, data2=label_y):\n",
    "    datasets = load_data(data1,data2)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    \n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')    \n",
    "     \n",
    "    #epoch=0 \n",
    "    #if epoch == 0:\n",
    "    #    WW=W_4to5\n",
    "\n",
    "    #print epoch\n",
    "    \n",
    "    classifier = MLP(input=x,\n",
    "                    n_in=373248*1,\n",
    "                    n_hidden4=n_hidden4,\n",
    "                    n_hidden=n_hidden,\n",
    "                    n_out=1)\n",
    "    \n",
    "    cost = (classifier.error(y)+0.0001 * classifier.WD)\n",
    "    \n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    \n",
    "    updates = [(param, param - learning_rate * gparam)\n",
    "              for param, gparam in zip(classifier.params, gparams)\n",
    "              ]\n",
    "    \n",
    "    train_model = theano.function(inputs=[index],\n",
    "                                 outputs=cost,\n",
    "                                 updates=updates,\n",
    "                                 givens={x:train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "                                         y:train_set_y[index*batch_size:(index+1)*batch_size]}\n",
    "                                 )\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training the model')\n",
    "    \n",
    "    epoch=0 \n",
    "    while (epoch<n_epochs):\n",
    "        epoch = epoch + 1\n",
    "        #print epoch\n",
    "        #if epoch != 1:\n",
    "        #    W=test_mlp2()\n",
    "            \n",
    "        for minibatch_index in range(n_train_batches): \n",
    "            minibatch_avg_cost=train_model(minibatch_index)    \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            #print iter\n",
    "            if (iter + 1) % 5 == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                losses = [train_model(i) for i in range(n_train_batches)]\n",
    "                this_loss = np.mean(losses)\n",
    "                \n",
    "                print \"Epoch {0}, Minibatch {1}/{2}, Test Error= {3}\".format(epoch,minibatch_index+1,n_train_batches,\n",
    "                                                                       this_loss)\n",
    "        print classifier.params[0].get_value()\n",
    "        #print classifier.params\n",
    "    return classifier.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... training the model\n"
     ]
    }
   ],
   "source": [
    "W=test_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "neurons = scipy.io.loadmat('input_neuron')\n",
    "neuron_input = neurons['WB']\n",
    "neuron_train = neuron_input[0:80]\n",
    "neuron_test = neuron_input[80:100]\n",
    "\n",
    "force = scipy.io.loadmat('true.mat')\n",
    "data_label=np.asarray(force['skt']).reshape(-1)\n",
    "label_y=(data_label-np.mean(data_label))/np.std(data_label)\n",
    "label_train=label_y[0:80]\n",
    "label_test=label_y[80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... training the model\n",
      "Epoch 1, Minibatch 5/10, Test Error= 0.809073169934\n",
      "Epoch 1, Minibatch 10/10, Test Error= 0.80969771361\n",
      "Epoch 2, Minibatch 5/10, Test Error= 0.844840856581\n",
      "Epoch 2, Minibatch 10/10, Test Error= 0.784064577676\n",
      "Epoch 3, Minibatch 5/10, Test Error= 0.769855442967\n",
      "Epoch 3, Minibatch 10/10, Test Error= 0.7196614645\n",
      "Epoch 4, Minibatch 5/10, Test Error= 0.725187087058\n",
      "Epoch 4, Minibatch 10/10, Test Error= 0.754380013179\n",
      "Epoch 5, Minibatch 5/10, Test Error= 0.771808806304\n",
      "Epoch 5, Minibatch 10/10, Test Error= 0.697811967509\n",
      "Epoch 6, Minibatch 5/10, Test Error= 0.69055509768\n",
      "Epoch 6, Minibatch 10/10, Test Error= 0.689813289471\n",
      "Epoch 7, Minibatch 5/10, Test Error= 0.715949443736\n",
      "Epoch 7, Minibatch 10/10, Test Error= 0.737578276114\n",
      "Epoch 8, Minibatch 5/10, Test Error= 0.719630432317\n",
      "Epoch 8, Minibatch 10/10, Test Error= 0.70926706878\n",
      "Epoch 9, Minibatch 5/10, Test Error= 0.725909838701\n",
      "Epoch 9, Minibatch 10/10, Test Error= 0.736575587448\n",
      "Epoch 10, Minibatch 5/10, Test Error= 0.743092060447\n",
      "Epoch 10, Minibatch 10/10, Test Error= 0.704361794724\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# 5to6 layer training #\n",
    "#######################\n",
    "class RBM_NN(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.W = theano.shared(value=np.zeros((n_in, n_out),dtype=theano.config.floatX),name='W',borrow=True)\n",
    "        self.b = theano.shared(value=np.zeros((n_out,),dtype=theano.config.floatX),name='b',borrow=True)               \n",
    "        self.y_pred = T.dot(input, self.W)+self.b      \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def difference(self,y):\n",
    "        return T.mean(abs(self.y_pred-T.reshape(y,[10,1])))\n",
    "        \n",
    "    \n",
    "def load_data(data1, data2):\n",
    "    #data1=neurons['xtr']\n",
    "    #data2=force['skt']\n",
    "    def shared_dataset(data1,data2,borrow=True):\n",
    "        shared_x = theano.shared(np.asarray(data1, dtype=theano.config.floatX),borrow=True)\n",
    "        shared_y = theano.shared(np.asarray(data2, dtype=theano.config.floatX),borrow=True)\n",
    "        return shared_x, T.cast(shared_y,'float64')\n",
    "    \n",
    "    train_set_x, train_set_y = shared_dataset(data1,data2)\n",
    "    rval = [(train_set_x, train_set_y)]\n",
    "    return rval\n",
    "\n",
    "def sgd_optimization(learning_rate=0.1, n_epochs=10, data1=neuron5, data2=label_y, batch_size=10):\n",
    "    datasets = load_data(data1, data2)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    \n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "        \n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')\n",
    "    \n",
    "    classifier = RBM_NN(input=x, n_in=30 * 1, n_out=1)\n",
    "    cost = classifier.difference(y)\n",
    "    \n",
    "    #test_model = theano.function(inputs=[index], outputs=cost,\n",
    "    #                            givens={x:test_set_x[index*batch_size:(index+1)*batch_size],\n",
    "    #                                   y:test_set_y[index*batch_size:(index+1)*batch_size]}\n",
    "    #                            )\n",
    "    \n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "    \n",
    "    updates = [(classifier.W, classifier.W-learning_rate*g_W),\n",
    "               (classifier.b, classifier.b-learning_rate*g_b)]\n",
    "    \n",
    "    #print cost\n",
    "    train_model = theano.function(inputs=[index],\n",
    "                                  outputs=cost,\n",
    "                                  updates=updates,\n",
    "                                 givens={x:train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "                                         y:train_set_y[index*batch_size:(index+1)*batch_size]}\n",
    "                                 ) \n",
    "\n",
    "    \n",
    "    test_model = theano.function(inputs=[index],\n",
    "                                outputs=classifier.error(y),\n",
    "                                givens={\n",
    "            x:test_set_x[index*batch_size:(index+1)*batch_size],\n",
    "            y:test_set_y[index*batch_size:(index+1)*batch_size]\n",
    "        })\n",
    "    \n",
    "    \n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training the model')\n",
    "    \n",
    "    epoch=0\n",
    "    while (epoch<n_epochs):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost=train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            #print iter\n",
    "            if (iter + 1) % 8 == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                losses = [train_model(i) for i in range(n_train_batches)]\n",
    "                this_loss = np.mean(losses)\n",
    "                \n",
    "                print \"Epoch {0}, Minibatch {1}/{2}, Test Error= {3}\".format(epoch,minibatch_index+1,n_train_batches,\n",
    "                                                                       this_loss)\n",
    "                \n",
    "            if epoch%10==0:\n",
    "                test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                print test_losses\n",
    "                test_score = np.mean(test_losses)\n",
    "            \n",
    "                print \"Testing Score= {0}\".format(test_score)\n",
    "                \n",
    "                \n",
    "        #print classifier.difference.y\n",
    "    return [classifier.W, classifier.b]\n",
    "# run\n",
    "W,b=sgd_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
