{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "import random\n",
    "\n",
    "#import input data\n",
    "neurons = scipy.io.loadmat('input_neuron')\n",
    "neuron_input = neurons['WB']\n",
    "neuron_train = neuron_input[0:80]\n",
    "neuron_test = neuron_input[80:100]\n",
    "\n",
    "force = scipy.io.loadmat('true.mat')\n",
    "data_label=np.asarray(force['skt']).reshape(-1)\n",
    "label_y=(data_label-np.mean(data_label))/np.std(data_label)\n",
    "label_train=label_y[0:80]\n",
    "label_test=label_y[80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RBM_NN(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.W = theano.shared(value=np.zeros((n_in, n_out),dtype=theano.config.floatX),name='W',borrow=True)\n",
    "        self.b = theano.shared(value=np.zeros((n_out,),dtype=theano.config.floatX),name='b',borrow=True)               \n",
    "        self.y_pred = T.dot(input, self.W)+self.b      \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def difference(self,y):\n",
    "        return T.mean(abs(self.y_pred-T.reshape(y,[10,1])))\n",
    "    \n",
    "def load_data(data1, data2):\n",
    "    def shared_dataset(data1,data2,borrow=True):\n",
    "        shared_x = theano.shared(np.asarray(data1, dtype=theano.config.floatX),borrow=True)\n",
    "        shared_y = theano.shared(np.asarray(data2, dtype=theano.config.floatX),borrow=True)\n",
    "        return shared_x, T.cast(shared_y,'float64')\n",
    "    \n",
    "    train_set_x, train_set_y = shared_dataset(data1,data2)\n",
    "    rval = [(train_set_x, train_set_y)]\n",
    "    return rval\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self,rng,input,n_in,n_out):\n",
    "        self.input=input\n",
    "        W_values = np.asarray(rng.uniform(low=-np.sqrt(6./(n_in+n_out)),\n",
    "                                          high=np.sqrt(6./(n_in+n_out)),\n",
    "                                          size=(n_in,n_out)),\n",
    "                              dtype=theano.config.floatX)\n",
    "        W = theano.shared(value=W_values, name='W',borrow=True)\n",
    "        \n",
    "        b_values = np.zeros((n_out,),dtype=theano.config.floatX)\n",
    "        b = theano.shared(value=b_values, name='b',borrow=True)\n",
    "        \n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        \n",
    "        lin_output = T.dot(input,self.W) + self.b\n",
    "        self.output = T.nnet.sigmoid(lin_output)\n",
    "        \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        \n",
    "class MLP(object):\n",
    "    def __init__(self,rng,input,n_in,n_hidden1,n_hidden2,n_out):\n",
    "        \n",
    "        self.hiddenLayer1 = HiddenLayer(rng,\n",
    "        input=input, n_in=n_in, n_out=n_hidden1)\n",
    "        \n",
    "        self.hiddenLayer2 = HiddenLayer(rng,\n",
    "        input=self.hiddenLayer1.output, n_in=n_hidden1, n_out=n_hidden2)\n",
    "        \n",
    "        self.LR = RBM_NN(\n",
    "        input=self.hiddenLayer2.output, n_in=n_hidden2, n_out=n_out)\n",
    "        \n",
    "        \n",
    "        self.error = self.LR.difference\n",
    "        \n",
    "        self.WD = abs(self.LR.W**2).sum() + abs(self.hiddenLayer1.W**2).sum() + abs(self.hiddenLayer2.W**2).sum()\n",
    "        \n",
    "        #self.error2 = abs(self.hiddenLayer.W).sum()+abs(self.layer6.W).sum()        \n",
    "        #self.error3 = abs(self.hiddenLayer3.W).sum()+abs(self.hiddenLayer.W).sum()+abs(self.layer6.W).sum()\n",
    "        \n",
    "        self.params = self.hiddenLayer1.params + self.hiddenLayer2.params\n",
    "        \n",
    "        self.input = input\n",
    "\n",
    "def test_mlp(learning_rate=0.01, n_epochs=1000,  data1=neuron_train, data2=label_train,\n",
    "             data3=neuron_test, data4=label_test, batch_size=10, n_hidden1=1000, n_hidden2=100):\n",
    "    datasets_train = load_data(data1,data2)\n",
    "    datasets_test = load_data(data3,data4)\n",
    "    \n",
    "    train_set_x, train_set_y = datasets_train[0]\n",
    "    test_set_x, test_set_y = datasets_test[0]\n",
    "    \n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0]//batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')    \n",
    "    \n",
    "    rng = np.random.RandomState(1234)\n",
    "    \n",
    "    classifier = MLP(rng=rng,\n",
    "                    input=x,\n",
    "                    n_in=200*200,\n",
    "                    n_hidden1=n_hidden1,\n",
    "                    n_hidden2=n_hidden2,\n",
    "                    n_out=1)\n",
    "    \n",
    "    cost=(classifier.error(y)+0.0001*classifier.WD)\n",
    "    \n",
    "    test_model = theano.function(inputs=[index],\n",
    "                                outputs=classifier.error(y),\n",
    "                                givens={\n",
    "            x:test_set_x[index*batch_size:(index+1)*batch_size],\n",
    "            y:test_set_y[index*batch_size:(index+1)*batch_size]\n",
    "        })\n",
    "    \n",
    "    gparams = [T.grad(cost,param) for param in classifier.params]\n",
    "    \n",
    "    updates = [(param, param - learning_rate*gparam) for \n",
    "              param, gparam in zip(classifier.params, gparams)]\n",
    "    \n",
    "    train_model = theano.function(inputs=[index],\n",
    "                                 outputs=cost,\n",
    "                                 updates=updates,\n",
    "                                 givens={\n",
    "            x: train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "            y: train_set_y[index*batch_size:(index+1)*batch_size]\n",
    "        })\n",
    "    \n",
    "    \n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training the model')\n",
    "    \n",
    "    epoch=0 \n",
    "    while (epoch<n_epochs):\n",
    "        epoch = epoch + 1\n",
    "        #print epoch\n",
    "        #if epoch != 1:\n",
    "        #    W=test_mlp2()\n",
    "            \n",
    "        for minibatch_index in range(n_train_batches): \n",
    "            minibatch_avg_cost=train_model(minibatch_index)    \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            #print iter\n",
    "            if (iter + 1) % 8 == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                losses = [train_model(i) for i in range(n_train_batches)]\n",
    "                this_loss = np.mean(losses)\n",
    "                \n",
    "                print \"Epoch {0}, Minibatch {1}/{2}, Test Error= {3}\".format(epoch,minibatch_index+1,n_train_batches,\n",
    "                                                                       this_loss)\n",
    "\n",
    "        if epoch%10==0:\n",
    "            test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "            print test_losses\n",
    "            test_score = np.mean(test_losses)\n",
    "            \n",
    "            print \"Testing Score= {0}\".format(test_score)\n",
    "            \n",
    "            \n",
    "    return classifier.params    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... training the model\n",
      "Epoch 1, Minibatch 8/8, Test Error= 0.959645017654\n",
      "Epoch 2, Minibatch 8/8, Test Error= 0.959631366543\n",
      "Epoch 3, Minibatch 8/8, Test Error= 0.959617716305\n",
      "Epoch 4, Minibatch 8/8, Test Error= 0.959604066942\n",
      "Epoch 5, Minibatch 8/8, Test Error= 0.959590418451\n",
      "Epoch 6, Minibatch 8/8, Test Error= 0.959576770834\n",
      "Epoch 7, Minibatch 8/8, Test Error= 0.959563124091\n",
      "Epoch 8, Minibatch 8/8, Test Error= 0.959549478221\n",
      "Epoch 9, Minibatch 8/8, Test Error= 0.959535833224\n",
      "Epoch 10, Minibatch 8/8, Test Error= 0.959522189101\n",
      "[array(0.8777249536776599), array(0.9567385766543373)]\n",
      "Testing Score= 0.917231765166\n",
      "Epoch 11, Minibatch 8/8, Test Error= 0.95950854585\n"
     ]
    }
   ],
   "source": [
    "W=test_mlp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
