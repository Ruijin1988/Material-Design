{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "import random\n",
    "\n",
    "# import 5th layer data\n",
    "force = scipy.io.loadmat('true.mat')\n",
    "neurons = scipy.io.loadmat('hidstates5th_WB_recon_30neuron')\n",
    "neuron5=neurons['xtr'].T\n",
    "data_label=np.asarray(force['skt']).reshape(-1)\n",
    "label_y=(data_label-np.mean(data_label))/np.std(data_label)\n",
    "\n",
    "#import 4th layer data\n",
    "neurons = scipy.io.loadmat('hidstates4th_WB_recon_1000neuron')\n",
    "neuron4=neurons['xtr']\n",
    "W_4to5=scipy.io.loadmat('W5.mat')['W5']\n",
    "\n",
    "#import 3rd layer data\n",
    "neurons = scipy.io.loadmat('hidstates3rd_WB_recon_373248neuron')\n",
    "neuron3=neurons['xtr']\n",
    "W_3to4=scipy.io.loadmat('W4.mat')['W4'] \n",
    "\n",
    "#import 2nd layer data\n",
    "neurons = scipy.io.loadmat('hidstates2nd_WB_recon_77440neuron')\n",
    "neuron2=neurons['pool']\n",
    "W_2to3=scipy.io.loadmat('W3.mat')['W3'].reshape(288,40,9,9) #in the right shape\n",
    "\n",
    "#import 1st layer data\n",
    "neurons = scipy.io.loadmat('hidstates1st_WB_recon_225816neuron')\n",
    "neuron1=neurons['pool']\n",
    "W_1to2=scipy.io.loadmat('W2.mat')['W2'].reshape(40,24,9,9) #in the right shape\n",
    "\n",
    "#import input data\n",
    "neurons = scipy.io.loadmat('input_neuron')\n",
    "neuron_input = neurons['WB']\n",
    "W_0to1 = scipy.io.loadmat('W1.mat')['W1'].reshape(24,1,6,6)  #in the right shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RBM_NN(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.W = theano.shared(value=np.zeros((n_in, n_out),dtype=theano.config.floatX),name='W',borrow=True)\n",
    "        self.b = theano.shared(value=np.zeros((n_out,),dtype=theano.config.floatX),name='b',borrow=True)               \n",
    "        self.y_pred = T.dot(input, self.W)+self.b      \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def difference(self,y):\n",
    "        return T.mean(abs(self.y_pred-T.reshape(y,[10,1])))\n",
    "    \n",
    "def load_data(data1, data2):\n",
    "    def shared_dataset(data1,data2,borrow=True):\n",
    "        shared_x = theano.shared(np.asarray(data1, dtype=theano.config.floatX),borrow=True)\n",
    "        shared_y = theano.shared(np.asarray(data2, dtype=theano.config.floatX),borrow=True)\n",
    "        return shared_x, T.cast(shared_y,'float64')\n",
    "    \n",
    "    train_set_x, train_set_y = shared_dataset(data1,data2)\n",
    "    rval = [(train_set_x, train_set_y)]\n",
    "    return rval\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self,input,n_in,n_out,WW):\n",
    "        self.input=input\n",
    "        W=np.asarray(WW,dtype=theano.config.floatX)\n",
    "        W=theano.shared(value=W, name='W',borrow=True)\n",
    "        #W_values = np.ones((n_in,n_out), dtype=theano.config.floatX)\n",
    "        #W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "                \n",
    "        b_value = np.zeros((n_out,),dtype=theano.config.floatX)\n",
    "        b = theano.shared(value=b_value, name='b',borrow=True)\n",
    "        \n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        \n",
    "        lin_output = T.dot(input,self.W) + self.b\n",
    "        self.output = T.nnet.sigmoid(lin_output)\n",
    "        \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "class ConvPoolLayer(object):\n",
    "    def __init__(self, input,filter_shape,image_shape, poolsize,WW):\n",
    "        self.input = input\n",
    "        self.W=theano.shared(np.asarray(WW,dtype=theano.config.floatX),\n",
    "                             borrow=True)\n",
    "        \n",
    "        b_values = np.zeros((filter_shape[0],),dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "        \n",
    "        conv_out = conv2d(input=input,\n",
    "                         filters=self.W,\n",
    "                         filter_shape=filter_shape,\n",
    "                         input_shape=image_shape)  \n",
    "        \n",
    "        pooled_out = pool.pool_2d(input = conv_out,\n",
    "                                 ds=poolsize,\n",
    "                                 ignore_border=True)\n",
    "        \n",
    "        \n",
    "        self.output = T.nnet.sigmoid(pooled_out+self.b.dimshuffle('x',0,'x','x'))\n",
    "        \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "        \n",
    "class MLP(object):\n",
    "    def __init__(self,input,n_in,n_hidden1, n_hidden2, n_hidden3, n_hidden4, n_hidden, n_out):\n",
    "        \n",
    "        self.inputLayer = ConvPoolLayer(input = input,\n",
    "                                       image_shape = (10,1,200,200),\n",
    "                                       filter_shape = (24,1,6,6),\n",
    "                                       poolsize = (2,2),\n",
    "                                       WW=W_0to1)\n",
    "        \n",
    "        self.hiddenLayer1 = ConvPoolLayer(input = self.inputLayer.output,\n",
    "                                         image_shape = (10,24,97,97),\n",
    "                                         filter_shape = (40,24,9,9),\n",
    "                                         poolsize = (2,2),\n",
    "                                         WW=W_1to2)    \n",
    "        \n",
    "        self.hiddenLayer2 = ConvPoolLayer(input = self.hiddenLayer1.output,\n",
    "                                          image_shape = (10,40,44,44),\n",
    "                                          filter_shape = (288,40,9,9),\n",
    "                                          poolsize=(1,1),\n",
    "                                          WW=W_2to3)\n",
    "        \n",
    "        self.layer3_input = self.hiddenLayer2.output.flatten(2)\n",
    "        #layer3_input = layer0.output.flatten(2)\n",
    "        \n",
    "        \n",
    "        self.hiddenLayer3 = HiddenLayer(\n",
    "        input=self.layer3_input, n_in=n_hidden3, n_out=n_hidden4,WW=W_3to4)\n",
    "\n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "        input=self.hiddenLayer3.output, n_in=n_hidden4, n_out=n_hidden,WW=W_4to5)\n",
    "        \n",
    "        self.layer6 = RBM_NN(\n",
    "        input=self.hiddenLayer.output, n_in=n_hidden, n_out=n_out)\n",
    "        \n",
    "        \n",
    "        self.error = self.layer6.difference\n",
    "        \n",
    "        self.WD = abs(self.inputLayer.W**2).sum() + abs(self.hiddenLayer1.W**2).sum() + abs(self.hiddenLayer2.W**2).sum() + \\\n",
    "                    abs(self.hiddenLayer3.W**2).sum() + abs(self.hiddenLayer.W**2).sum() + abs(self.layer6.W**2).sum()\n",
    "        \n",
    "        #self.error2 = abs(self.hiddenLayer.W).sum()+abs(self.layer6.W).sum()        \n",
    "        #self.error3 = abs(self.hiddenLayer3.W).sum()+abs(self.hiddenLayer.W).sum()+abs(self.layer6.W).sum()\n",
    "        \n",
    "        self.params = self.inputLayer.params + self.hiddenLayer1.params + self.hiddenLayer2.params + \\\n",
    "                        self.hiddenLayer3.params + self.hiddenLayer.params + self.layer6.params\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "def test_mlp(learning_rate=0.1, n_epochs=2000, batch_size=10, n_hidden1=97*97*24, n_hidden2=44*44*40, n_hidden3=36*36*288, \n",
    "             n_hidden4=1000, n_hidden=30, data1=neuron_input, data2=label_y):\n",
    "    datasets = load_data(data1,data2)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    \n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    \n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print('... building the model')\n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    y = T.vector('y')    \n",
    "    \n",
    "    layer0_input = x.reshape((batch_size,1, 200, 200))\n",
    "    \n",
    "    #layer2 = ConvPoolLayer(input = layer2_input,\n",
    "    #                      image_shape = (batch_size,40,44,44),\n",
    "    #                      filter_shape = (288,40,9,9))\n",
    "    \n",
    "    #layer3_input = layer0.output.flatten(2)\n",
    "    \n",
    "    #print('-----check')\n",
    "    #return layer3_input\n",
    "    \n",
    "    classifier = MLP(input=layer0_input,\n",
    "                    n_in=200*200*1,\n",
    "                    n_hidden1 = n_hidden1,\n",
    "                    n_hidden2 = n_hidden2,\n",
    "                    n_hidden3 = n_hidden3,\n",
    "                    n_hidden4 = n_hidden4,\n",
    "                    n_hidden = n_hidden,\n",
    "                    n_out=1)\n",
    "    \n",
    "    \n",
    "    cost = (classifier.error(y)+0.0001 * classifier.WD)\n",
    "    \n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    \n",
    "    updates = [(param, param - learning_rate * gparam)\n",
    "              for param, gparam in zip(classifier.params, gparams)\n",
    "              ]\n",
    "    \n",
    "    train_model = theano.function(inputs=[index],\n",
    "                                 outputs=cost,\n",
    "                                 updates=updates,\n",
    "                                 givens={x:train_set_x[index*batch_size:(index+1)*batch_size],\n",
    "                                         y:train_set_y[index*batch_size:(index+1)*batch_size]}\n",
    "                                 )\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print('... training the model')\n",
    "    \n",
    "    epoch=0 \n",
    "    while (epoch<n_epochs):\n",
    "        epoch = epoch + 1\n",
    "        #print epoch\n",
    "        #if epoch != 1:\n",
    "        #    W=test_mlp2()\n",
    "            \n",
    "        for minibatch_index in range(n_train_batches): \n",
    "            minibatch_avg_cost=train_model(minibatch_index)    \n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            #print iter\n",
    "            if (iter + 1) % 5 == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                losses = [train_model(i) for i in range(n_train_batches)]\n",
    "                this_loss = np.mean(losses)\n",
    "                \n",
    "                print \"Epoch {0}, Minibatch {1}/{2}, Test Error= {3}\".format(epoch,minibatch_index+1,n_train_batches,\n",
    "                                                                       this_loss)\n",
    "        #print classifier.params[0].get_value()\n",
    "        #print classifier.params\n",
    "    return classifier.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n"
     ]
    }
   ],
   "source": [
    "W=test_mlp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
